import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np
import os
import albumentations as A
from albumentations.pytorch import ToTensorV2
from model import HumanSegmentator
import segmentation_models_pytorch as smp

class SegmentationDataset(Dataset):
    def __init__(self, images_dir, masks_dir, size=(512, 512)):
        self.images_dir = images_dir
        self.masks_dir = masks_dir
        self.size = size
        self.image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
        
        self.transform = A.Compose([
            A.Resize(size[0], size[1]),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.2),
            A.RandomBrightnessContrast(p=0.3),
            A.GaussianBlur(blur_limit=3, p=0.2),
            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),
            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ToTensorV2(),
        ])
        
        self.mask_transform = A.Compose([
            A.Resize(size[0], size[1]),
            ToTensorV2(),
        ])
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.images_dir, img_name)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—â–µ–º –º–∞—Å–∫—É
        mask_name = img_name.replace('.jpg', '.png').replace('.jpeg', '.png')
        mask_path = os.path.join(self.masks_dir, mask_name)
        
        if not os.path.exists(mask_path):
            mask_path = os.path.join(self.masks_dir, 'mask_' + mask_name)
        
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        
        if image is None or mask is None:
            return self.__getitem__((idx + 1) % len(self.image_files))
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        transformed = self.transform(image=image)
        image_tensor = transformed['image']
        
        mask_transformed = self.mask_transform(image=mask)
        mask_tensor = mask_transformed['image'].float() / 255.0
        
        return image_tensor, mask_tensor

def download_sample_dataset():
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞"""
    import urllib.request
    import zipfile
    
    # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏-–¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    os.makedirs('train_data/images', exist_ok=True)
    os.makedirs('train_data/masks', exist_ok=True)
    
    print("–°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    for i in range(50):
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img = np.random.randint(50, 200, (512, 512, 3), dtype=np.uint8)
        
        # –°–æ–∑–¥–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –º–∞—Å–∫—É
        mask = np.zeros((512, 512), dtype=np.uint8)
        
        # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Å–∏–ª—É—ç—Ç —á–µ–ª–æ–≤–µ–∫–∞
        cv2.ellipse(mask, (256, 150), (80, 100), 0, 0, 360, 255, -1)  # –ì–æ–ª–æ–≤–∞
        cv2.rectangle(mask, (176, 150), (336, 350), 255, -1)  # –¢–µ–ª–æ
        cv2.rectangle(mask, (176, 350), (216, 450), 255, -1)  # –ù–æ–≥–∞ –ª–µ–≤–∞—è
        cv2.rectangle(mask, (296, 350), (336, 450), 255, -1)  # –ù–æ–≥–∞ –ø—Ä–∞–≤–∞—è
        cv2.rectangle(mask, (136, 180), (176, 300), 255, -1)  # –†—É–∫–∞ –ª–µ–≤–∞—è
        cv2.rectangle(mask, (336, 180), (376, 300), 255, -1)  # –†—É–∫–∞ –ø—Ä–∞–≤–∞—è
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
        noise = np.random.randint(0, 50, (512, 512), dtype=np.uint8)
        mask = cv2.add(mask, noise)
        mask = cv2.GaussianBlur(mask, (5, 5), 0)
        mask = (mask > 128).astype(np.uint8) * 255
        
        cv2.imwrite(f'train_data/images/image_{i:03d}.jpg', img)
        cv2.imwrite(f'train_data/masks/mask_{i:03d}.png', mask)
    
    print("‚úÖ –°–æ–∑–¥–∞–Ω–æ 50 —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")

def train_model():
    print("üöÄ –ù–∞—á–∞–ª–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    model = smp.Unet(
        encoder_name="resnet34",
        encoder_weights="imagenet",
        classes=1,
        activation="sigmoid"
    ).to(device)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
    if os.path.exists('improved_model.pth'):
        model.load_state_dict(torch.load('improved_model.pth', map_location=device))
        print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–µ—Å–∞")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    if not os.path.exists('train_data'):
        download_sample_dataset()
    
    dataset = SegmentationDataset('train_data/images', 'train_data/masks')
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    criterion = nn.BCEWithLogitsLoss()
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
    
    print("üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    model.train()
    
    for epoch in range(20):
        total_loss = 0
        for batch_idx, (images, masks) in enumerate(dataloader):
            images, masks = images.to(device), masks.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if batch_idx % 5 == 0:
                print(f'–≠–ø–æ—Ö–∞ {epoch+1}, –ë–∞—Ç—á {batch_idx}, Loss: {loss.item():.4f}')
        
        scheduler.step()
        avg_loss = total_loss / len(dataloader)
        print(f'‚úÖ –≠–ø–æ—Ö–∞ {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω–∏–π Loss: {avg_loss:.4f}')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
        if (epoch + 1) % 5 == 0:
            torch.save(model.state_dict(), f'improved_model_epoch_{epoch+1}.pth')
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    torch.save(model.state_dict(), 'improved_model.pth')
    print("üíæ –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'improved_model.pth'")

if __name__ == "__main__":
    train_model()