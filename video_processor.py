import cv2
import torch
import numpy as np
import time
from model import HumanSegmentator

class VideoProcessor:
    def __init__(self, model_path=None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üé• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∏–¥–µ–æ-–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –Ω–∞ {self.device}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        self.segmentator = HumanSegmentator()
        if model_path and os.path.exists(model_path):
            self.segmentator.model.load_state_dict(torch.load(model_path, map_location=self.device))
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.optimize_for_speed()
        
        self.cap = None
        self.is_processing = False
        self.current_background = "blue"
        self.custom_background = None
        
    def optimize_for_speed(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        # TorchScript –∫–æ–º–ø–∏–ª—è—Ü–∏—è
        try:
            self.segmentator.model = torch.jit.script(self.segmentator.model)
            print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞ —Å TorchScript")
        except:
            print("‚ö†Ô∏è TorchScript –∫–æ–º–ø–∏–ª—è—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º")
        
        # –ü–æ–ª–æ–≤–∏–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è GPU
        if self.device.type == 'cuda':
            self.segmentator.model = self.segmentator.model.half()
            print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª–æ–≤–∏–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (FP16)")
        
        # –í–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ –æ—Ü–µ–Ω–∫–∏
        self.segmentator.model.eval()
        
    def start_webcam(self, camera_id=0, output_size=(1280, 720)):
        """–ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã"""
        self.cap = cv2.VideoCapture(camera_id)
        
        if not self.cap.isOpened():
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É {camera_id}")
            return False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–º–µ—Ä—ã –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, output_size[0])
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, output_size[1])
        self.cap.set(cv2.CAP_PROP_FPS, 30)
        
        self.is_processing = True
        print("üé• –í–µ–±-–∫–∞–º–µ—Ä–∞ –∑–∞–ø—É—â–µ–Ω–∞. –ù–∞–∂–º–∏ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞, 'b' –¥–ª—è —Å–º–µ–Ω—ã —Ñ–æ–Ω–∞")
        
        fps_counter = 0
        fps_time = time.time()
        
        while self.is_processing:
            ret, frame = self.cap.read()
            if not ret:
                break
            
            # –ò–∑–º–µ—Ä–µ–Ω–∏–µ FPS
            fps_counter += 1
            if time.time() - fps_time >= 1.0:
                fps = fps_counter
                fps_counter = 0
                fps_time = time.time()
                print(f"üìä FPS: {fps}")
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞
            processed_frame = self.process_frame(frame)
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ FPS –Ω–∞ –∫–∞–¥—Ä–µ
            cv2.putText(processed_frame, f"FPS: {fps}", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(processed_frame, f"Background: {self.current_background}", (10, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            cv2.imshow('AI Human Segmentation - Video', processed_frame)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞–≤–∏—à
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('b'):
                self.cycle_background()
            elif key == ord('c'):
                self.use_custom_background()
        
        self.stop_webcam()
    
    def process_frame(self, frame):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        try:
            with torch.no_grad():  # –û—Ç–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                # –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                mask, result = self.segmentator.process_image(frame, self.current_background)
                return cv2.cvtColor(result, cv2.COLOR_RGB2BGR)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–∞: {e}")
            return frame
    
    def cycle_background(self):
        """–°–º–µ–Ω–∞ —Ñ–æ–Ω–∞"""
        backgrounds = ["blue", "green", "gradient", "black", "blur"]
        current_idx = backgrounds.index(self.current_background) if self.current_background in backgrounds else 0
        self.current_background = backgrounds[(current_idx + 1) % len(backgrounds)]
        print(f"üé® –°–º–µ–Ω–µ–Ω —Ñ–æ–Ω –Ω–∞: {self.current_background}")
    
    def use_custom_background(self):
        """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Ñ–æ–Ω–∞"""
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É —Å–≤–æ–µ–≥–æ —Ñ–æ–Ω–∞
        print("üñºÔ∏è –§—É–Ω–∫—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Ñ–æ–Ω–∞ (—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)")
    
    def stop_webcam(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–µ–±-–∫–∞–º–µ—Ä—ã"""
        self.is_processing = False
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()
        print("üõë –í–∏–¥–µ–æ-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

def main():
    processor = VideoProcessor(model_path='improved_model.pth')
    processor.start_webcam()

if __name__ == "__main__":
    main()