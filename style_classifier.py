import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import cv2
import numpy as np

class StyleClassifier:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self._create_model()
        self.transform = self._get_transform()
        
        # –°—Ç–∏–ª–∏ –æ–¥–µ–∂–¥—ã
        self.style_labels = [
            "–¥–µ–ª–æ–≤–æ–π", "–ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–π", "—Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–π", "—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π", "—É–ª–∏—á–Ω—ã–π"
        ]
        
        # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∏–ª–µ–π —Ñ–æ–Ω–∞–º
        self.style_to_background = {
            "–¥–µ–ª–æ–≤–æ–π": "–æ—Ñ–∏—Å",
            "–ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–π": "–∫–∞—Ñ–µ", 
            "—Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–π": "—Å–ø–æ—Ä—Ç–∑–∞–ª",
            "—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π": "—Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π",
            "—É–ª–∏—á–Ω—ã–π": "–≥–æ—Ä–æ–¥"
        }
    
    def _create_model(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ç–∏–ª—è"""
        model = models.resnet18(pretrained=True)
        model.fc = nn.Linear(model.fc.in_features, len(self.style_labels))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
        try:
            model.load_state_dict(torch.load('style_classifier.pth', map_location=self.device))
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ç–∏–ª–µ–π")
        except:
            print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ç–∏–ª–µ–π")
        
        model = model.to(self.device)
        model.eval()
        return model
    
    def _get_transform(self):
        """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def predict_style(self, image):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–∏–ª—è –æ–¥–µ–∂–¥—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
        if isinstance(image, str):
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞
            image = Image.open(image).convert('RGB')
        elif isinstance(image, np.ndarray):
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑ OpenCV
            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        
        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        input_tensor = self.transform(image).unsqueeze(0).to(self.device)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            predicted_idx = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][predicted_idx].item()
        
        predicted_style = self.style_labels[predicted_idx]
        
        return predicted_style, confidence
    
    def get_recommended_background(self, image):
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Ñ–æ–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∏–ª—è"""
        style, confidence = self.predict_style(image)
        
        if confidence < 0.5:
            return "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π", style, confidence
        
        recommended_bg = self.style_to_background.get(style, "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π")
        return recommended_bg, style, confidence

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–∏–¥–µ–æ-–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º
class SmartVideoProcessor(VideoProcessor):
    def __init__(self, model_path=None):
        super().__init__(model_path)
        self.style_classifier = StyleClassifier()
        self.last_style_check = 0
        self.style_check_interval = 5  # —Å–µ–∫—É–Ω–¥
    
    def process_frame_smart(self, frame):
        """–£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ —Å –∞–≤—Ç–æ-–ø–æ–¥–±–æ—Ä–æ–º —Ñ–æ–Ω–∞"""
        current_time = time.time()
        
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∏–ª—å
        if current_time - self.last_style_check > self.style_check_interval:
            recommended_bg, style, confidence = self.style_classifier.get_recommended_background(frame)
            
            if confidence > 0.6:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                self.current_background = self.map_background_type(recommended_bg)
                print(f"üé® –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä: {style} -> {recommended_bg} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            
            self.last_style_check = current_time
        
        # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        return self.process_frame(frame)
    
    def map_background_type(self, background_name):
        """–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–æ–Ω–∞ —Å —Ç–∏–ø–æ–º"""
        background_mapping = {
            "–æ—Ñ–∏—Å": "gradient",
            "–∫–∞—Ñ–µ": "green", 
            "—Å–ø–æ—Ä—Ç–∑–∞–ª": "blue",
            "—Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π": "gradient",
            "–≥–æ—Ä–æ–¥": "black",
            "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π": "blue"
        }
        return background_mapping.get(background_name, "blue")

# –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
def train_quick_classifier():
    """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å—Ç–∏–ª–µ–π"""
# –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–º–æ
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å—Ç–∏–ª–µ–π...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
    model = models.resnet18(pretrained=True)
    model.fc = nn.Linear(model.fc.in_features, 5)
    torch.save(model.state_dict(), 'style_classifier.pth')
    print("‚úÖ –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω")